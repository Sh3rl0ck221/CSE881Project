{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests, os, sys, tarfile\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# url = \"http://labrosa.ee.columbia.edu/~dpwe/tmp/millionsongsubset.tar.gz\"\n",
    "# archive_name = \"millionsongsubset.tar.gz\"\n",
    "# folder_name = \"millionsongsubset\"\n",
    "\n",
    "# def download_file(url, archive_name):\n",
    "#     response = requests.get(url, stream=True)\n",
    "#     total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "#     block_size = 1024\n",
    "#     progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "#     with open(archive_name, 'wb') as file:\n",
    "#         for data in response.iter_content(block_size):\n",
    "#             progress_bar.update(len(data))\n",
    "#             file.write(data)\n",
    "#     progress_bar.close()\n",
    "#     if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "#         print(\"ERROR, something went wrong\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "# def extract_file(archive_name, folder_name):\n",
    "#     with tarfile.open(archive_name, 'r:gz') as tar:\n",
    "#         tar.extractall(folder_name)\n",
    "\n",
    "# if not os.path.exists(folder_name):\n",
    "#     download_file(url, archive_name)\n",
    "# if not os.path.exists(folder_name):\n",
    "#     extract_file(archive_name, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['analysis', 'metadata', 'musicbrainz']>\n",
      "<KeysViewHDF5 ['artist_terms', 'artist_terms_freq', 'artist_terms_weight', 'similar_artists', 'songs']>\n",
      "<KeysViewHDF5 ['bars_confidence', 'bars_start', 'beats_confidence', 'beats_start', 'sections_confidence', 'sections_start', 'segments_confidence', 'segments_loudness_max', 'segments_loudness_max_time', 'segments_loudness_start', 'segments_pitches', 'segments_start', 'segments_timbre', 'songs', 'tatums_confidence', 'tatums_start']>\n",
      "analyzer_version [b'']\n",
      "artist_7digitalid [165270]\n",
      "artist_familiarity [0.58179377]\n",
      "artist_hotttnesss [0.40199754]\n",
      "artist_id [b'ARD7TVE1187B99BFB1']\n",
      "artist_latitude [nan]\n",
      "artist_location [b'California - LA']\n",
      "artist_longitude [nan]\n",
      "artist_mbid [b'e77e51a5-4761-45b3-9847-2051f811e366']\n",
      "artist_name [b'Casual']\n",
      "artist_playmeid [4479]\n",
      "genre [b'']\n",
      "idx_artist_terms [0]\n",
      "idx_similar_artists [0]\n",
      "release [b'Fear Itself']\n",
      "release_7digitalid [300848]\n",
      "song_hotttnesss [0.60211999]\n",
      "song_id [b'SOMZWCG12A8C13C480']\n",
      "title [b\"I Didn't Mean To\"]\n",
      "track_7digitalid [3401791]\n",
      "b'SOMZWCG12A8C13C480'\n"
     ]
    }
   ],
   "source": [
    "# # Explore the data\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# sub_folder = \"MillionSongSubset\"\n",
    "\n",
    "# filename = \"A/A/A/TRAAAAW128F429D538.h5\"\n",
    "# filename = os.path.join(folder_name, sub_folder, filename)\n",
    "# h5 = h5py.File(filename, 'r')\n",
    "# print(h5.keys())\n",
    "# print(h5['metadata'].keys())\n",
    "# print(h5['analysis'].keys())\n",
    "\n",
    "# # Show fields in songs\n",
    "# # print(list(h5['metadata']['songs'].dtype.fields.keys()))\n",
    "# # print(list(h5['metadata']['songs']))\n",
    "# keys = list(h5['metadata']['songs'].dtype.fields.keys())\n",
    "# for key in keys:\n",
    "#     print(key, h5['metadata']['songs'][key])\n",
    "# # Show song id\n",
    "# print(h5['metadata']['songs']['song_id'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Walk through all files and add song_id and features to a pandas dataframe\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def get_dataframe(basedir, ext='.h5'):\n",
    "#     metadata_data = []\n",
    "#     analysis_data = []\n",
    "#     for root, _, files in tqdm(os.walk(basedir), total=932):\n",
    "#         files = glob.glob(os.path.join(root, f'*{ext}'))\n",
    "#         for file in files:\n",
    "#             h5 = h5py.File(file, 'r')\n",
    "#             metadata = h5['metadata']\n",
    "#             keys = list(metadata['songs'].dtype.fields.keys())\n",
    "#             assert len(metadata['songs']) == 1\n",
    "#             values = list(metadata['songs'][0])\n",
    "#             row = dict(zip(keys, values))\n",
    "#             row['song_id'] = row['song_id'].decode('utf-8')\n",
    "#             song_id = row['song_id']\n",
    "#             metadata_data.append(row)\n",
    "\n",
    "#             analysis = h5['analysis']\n",
    "#             keys = list(analysis['songs'].dtype.fields.keys())\n",
    "#             values = list(analysis['songs'][0])\n",
    "#             row = dict(zip(keys, values))\n",
    "#             row['song_id'] = song_id\n",
    "#             analysis_data.append(row)\n",
    "#             h5.close()\n",
    "\n",
    "#     metadata_data = pd.DataFrame(metadata_data)\n",
    "#     analysis_data = pd.DataFrame(analysis_data)\n",
    "#     # Move song_id to the first column\n",
    "#     metadata_data = metadata_data[['song_id'] + [col for col in metadata_data.columns if col != 'song_id']]\n",
    "#     analysis_data = analysis_data[['song_id'] + [col for col in analysis_data.columns if col != 'song_id']]\n",
    "#     return metadata_data, analysis_data\n",
    "\n",
    "# metadata_df_path = \"metadata.csv\"\n",
    "# analysis_df_path = \"analysis.csv\"\n",
    "# path = os.path.join(folder_name, sub_folder)\n",
    "\n",
    "# if not os.path.exists(metadata_df_path) or not os.path.exists(analysis_df_path):\n",
    "#     print(\"Walking through directory\", path)\n",
    "#     metadata_df, analysis_df = get_dataframe(folder_name)\n",
    "#     metadata_df.to_csv(metadata_df_path)\n",
    "#     analysis_df.to_csv(analysis_df_path)\n",
    "# else:\n",
    "#     metadata_df = pd.read_csv(metadata_df_path)\n",
    "#     analysis_df = pd.read_csv(analysis_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load train triplets\n",
    "filename = \"train_triplets.txt\"\n",
    "\n",
    "def get_triplets(filename):\n",
    "    triplets = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in tqdm(file, total=48373586):\n",
    "            user_id, song_id, play_count = line.strip().split('\\t')\n",
    "            triplets.append((user_id, song_id, int(play_count)))\n",
    "    return triplets\n",
    "\n",
    "triplets_path = \"triplets.csv\"\n",
    "if not os.path.exists(triplets_path):\n",
    "    triplets = get_triplets(filename)\n",
    "    triplets_df = pd.DataFrame(triplets, columns=['user_id', 'song_id', 'play_count'])\n",
    "    triplets_df.to_csv(triplets_path)\n",
    "else:\n",
    "    triplets_df = pd.read_csv(triplets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EASE\n",
    "\n",
    "model = EASE()\n",
    "# Rename the columns to user_id and item_id and ratings\n",
    "triplets_df.columns = ['user_id', 'item_id', 'rating']\n",
    "# Use a subset of the data\n",
    "N = 10000\n",
    "triplets_df = triplets_df[:N]\n",
    "model.fit(triplets_df, implicit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MSU\\MastersFall\\CSE881\\project\\model.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dd['ci'] = self.item_enc.transform(dd.item_id)\n",
      "d:\\MSU\\MastersFall\\CSE881\\project\\model.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dd['cu'] = self.user_enc.transform(dd.user_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssit5\\AppData\\Local\\Temp\\ipykernel_3812\\1245215593.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1 = 2 * (precision * recall) / (precision + recall)\n"
     ]
    }
   ],
   "source": [
    "# Measure the performance of the model\n",
    "\n",
    "# def predict(self, train, users, items, k):\n",
    "#         items = self.item_enc.transform(items)\n",
    "#         dd = train.loc[train.user_id.isin(users)]\n",
    "#         dd['ci'] = self.item_enc.transform(dd.item_id)\n",
    "#         dd['cu'] = self.user_enc.transform(dd.user_id)\n",
    "#         g = dd.groupby('cu')\n",
    "#         with Pool(cpu_count()) as p:\n",
    "#             user_preds = p.starmap(\n",
    "#                 self.predict_for_user,\n",
    "#                 [(user, group, self.pred[user, :], items, k) for user, group in g],\n",
    "#             )\n",
    "#         df = pd.concat(user_preds)\n",
    "#         df['item_id'] = self.item_enc.inverse_transform(df['item_id'])\n",
    "#         df['user_id'] = self.user_enc.inverse_transform(df['user_id'])\n",
    "#         return df\n",
    "\n",
    "def precision_at_k(predictions, k):\n",
    "    predictions = predictions.sort_values('score', ascending=False)\n",
    "    predictions = predictions[:k]\n",
    "    return len(predictions[predictions['score'] > 0]) / k\n",
    "\n",
    "def recall_at_k(predictions, k):\n",
    "    predictions = predictions.sort_values('score', ascending=False)\n",
    "    predictions = predictions[:k]\n",
    "    return len(predictions[predictions['score'] > 0]) / len(predictions)\n",
    "\n",
    "def evaluate(model, triplets_df, N=10, k=10):\n",
    "    users = triplets_df.user_id.unique()\n",
    "    items = triplets_df.item_id.unique()\n",
    "    users = users[:N]\n",
    "    items = items[:N]\n",
    "    predictions = model.predict(triplets_df, users, items, k)\n",
    "\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for user in users:\n",
    "        user_predictions = predictions[predictions['user_id'] == user]\n",
    "        precision_scores.append(precision_at_k(user_predictions, k))\n",
    "        recall_scores.append(recall_at_k(user_predictions, k))\n",
    "        \n",
    "    precision = np.mean(precision_scores)\n",
    "    recall = np.mean(recall_scores)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "precision, recall, f1 = evaluate(model, triplets_df, N=100, k=10)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x7849 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = model.X\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>song_id</th>\n",
       "      <th>play_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n",
       "      <td>SOAKIMP12A8C130995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n",
       "      <td>SOAPDEY12A81C210A9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n",
       "      <td>SOBBMDR12A8C13253B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n",
       "      <td>SOBFNSP12AF72A0E22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n",
       "      <td>SOBFOVM12A58A7D494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48373581</th>\n",
       "      <td>48373581</td>\n",
       "      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n",
       "      <td>SOUHHHH12AF729E4AF</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48373582</th>\n",
       "      <td>48373582</td>\n",
       "      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n",
       "      <td>SOUJVIT12A8C1451C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48373583</th>\n",
       "      <td>48373583</td>\n",
       "      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n",
       "      <td>SOUSMXX12AB0185C24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48373584</th>\n",
       "      <td>48373584</td>\n",
       "      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n",
       "      <td>SOWYSKH12AF72A303A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48373585</th>\n",
       "      <td>48373585</td>\n",
       "      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n",
       "      <td>SOYYFLV12A58A7A88F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48373586 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0                                   user_id  \\\n",
       "0                  0  b80344d063b5ccb3212f76538f3d9e43d87dca9e   \n",
       "1                  1  b80344d063b5ccb3212f76538f3d9e43d87dca9e   \n",
       "2                  2  b80344d063b5ccb3212f76538f3d9e43d87dca9e   \n",
       "3                  3  b80344d063b5ccb3212f76538f3d9e43d87dca9e   \n",
       "4                  4  b80344d063b5ccb3212f76538f3d9e43d87dca9e   \n",
       "...              ...                                       ...   \n",
       "48373581    48373581  b7815dbb206eb2831ce0fe040d0aa537e2e800f7   \n",
       "48373582    48373582  b7815dbb206eb2831ce0fe040d0aa537e2e800f7   \n",
       "48373583    48373583  b7815dbb206eb2831ce0fe040d0aa537e2e800f7   \n",
       "48373584    48373584  b7815dbb206eb2831ce0fe040d0aa537e2e800f7   \n",
       "48373585    48373585  b7815dbb206eb2831ce0fe040d0aa537e2e800f7   \n",
       "\n",
       "                     song_id  play_count  \n",
       "0         SOAKIMP12A8C130995           1  \n",
       "1         SOAPDEY12A81C210A9           1  \n",
       "2         SOBBMDR12A8C13253B           2  \n",
       "3         SOBFNSP12AF72A0E22           1  \n",
       "4         SOBFOVM12A58A7D494           1  \n",
       "...                      ...         ...  \n",
       "48373581  SOUHHHH12AF729E4AF           2  \n",
       "48373582  SOUJVIT12A8C1451C1           1  \n",
       "48373583  SOUSMXX12AB0185C24           1  \n",
       "48373584  SOWYSKH12AF72A303A           3  \n",
       "48373585  SOYYFLV12A58A7A88F           1  \n",
       "\n",
       "[48373586 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 01:40:59 [INFO] notebook - Building user + item lookup\n",
      "2024-11-01 01:40:59 [INFO] notebook - Building item hashmap\n",
      "2024-11-01 01:40:59 [INFO] notebook - User + item lookup complete\n",
      "2024-11-01 01:40:59 [INFO] notebook - Sparse data built\n",
      "2024-11-01 01:40:59 [INFO] notebook - Building G Matrix\n",
      "2024-11-01 01:40:59 [INFO] notebook - Building B matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MSU\\MastersFall\\CSE881\\project\\torch_ease.py:63: UserWarning: torch.sparse.SparseTensor(indices, values, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:631.)\n",
      "  self.sparse = torch.sparse.FloatTensor(self.indices.t(), self.values)\n"
     ]
    }
   ],
   "source": [
    "from torch_ease import TorchEASE\n",
    "\n",
    "model = TorchEASE(train=triplets_df[:1000], user_col='user_id', item_col='song_id', score_col='play_count')\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      user_id\n",
      "0    b80344d063b5ccb3212f76538f3d9e43d87dca9e\n",
      "104  85c1f87fea955d09b4bec2e36aee110927aedf9a\n",
      "125  bd4c6e843f00bd476847fb75c47b4fb430a06856\n",
      "136  8937134734f869debcab8f23d77465b4caaa85df\n",
      "149  969cc6fb74e076a68e36a04409cb9d3765757508\n",
      "182  4bd88bfb25263a75bbdd467e74018f4ae570e5df\n",
      "199  e006b1a48f466bf59feefed32bec6494495a4436\n",
      "227  9d6f0ead607ac2a6c2460e4d14fb439a146b7dec\n",
      "238  9bb911319fbc04f01755814cb5edb21df3d1a336\n",
      "252  b64cdd1a0bd907e5e00b39e345194768e330d652\n",
      "361  17aa9f6dbdf753831da8f38c71b66b64373de613\n",
      "498  d6589314c0a9bcbca4fee0c93b14bc402363afea\n",
      "558  5a905f000fc1ff3df7ca807d57edb608863db05d\n",
      "2024-11-01 01:42:12 [INFO] notebook - Predictions are made\n",
      "2024-11-01 01:42:12 [INFO] notebook - Removing owned items\n",
      "2024-11-01 01:42:12 [INFO] notebook - TopK selected per user\n",
      "2024-11-01 01:42:12 [INFO] notebook - Predictions are returned to user\n",
      "                                     user_id  user_id_id  \\\n",
      "0   b80344d063b5ccb3212f76538f3d9e43d87dca9e           9   \n",
      "1   85c1f87fea955d09b4bec2e36aee110927aedf9a           3   \n",
      "2   bd4c6e843f00bd476847fb75c47b4fb430a06856          10   \n",
      "3   8937134734f869debcab8f23d77465b4caaa85df           4   \n",
      "4   969cc6fb74e076a68e36a04409cb9d3765757508           5   \n",
      "5   4bd88bfb25263a75bbdd467e74018f4ae570e5df           1   \n",
      "6   e006b1a48f466bf59feefed32bec6494495a4436          12   \n",
      "7   9d6f0ead607ac2a6c2460e4d14fb439a146b7dec           7   \n",
      "8   9bb911319fbc04f01755814cb5edb21df3d1a336           6   \n",
      "9   b64cdd1a0bd907e5e00b39e345194768e330d652           8   \n",
      "10  17aa9f6dbdf753831da8f38c71b66b64373de613           0   \n",
      "11  d6589314c0a9bcbca4fee0c93b14bc402363afea          11   \n",
      "12  5a905f000fc1ff3df7ca807d57edb608863db05d           2   \n",
      "\n",
      "                                      predicted_items  \n",
      "0   [SOWUVFQ12AB018740E, SORMKFZ12A6D4F9395, SOGYN...  \n",
      "1   [SOABRAB12A6D4F7AAF, SOADGFH12A8C143D89, SOAAR...  \n",
      "2   [SOMVTRL12A67AE0921, SOAMPRJ12A8AE45F38, SOVHV...  \n",
      "3   [SOMVTRL12A67AE0921, SOAMPRJ12A8AE45F38, SOQTC...  \n",
      "4   [SOBONKR12A58A7A7E0, SOTTNZU12A6D4FA237, SOWNN...  \n",
      "5   [SOMVTRL12A67AE0921, SOAMPRJ12A8AE45F38, SOVHV...  \n",
      "6   [SOTTNZU12A6D4FA237, SOWNNPR12A6D4FB51B, SOPSO...  \n",
      "7   [SOMVTRL12A67AE0921, SOAMPRJ12A8AE45F38, SOVHV...  \n",
      "8   [SOMVTRL12A67AE0921, SOAMPRJ12A8AE45F38, SOVHV...  \n",
      "9   [SOMVTRL12A67AE0921, SOAMPRJ12A8AE45F38, SOVHV...  \n",
      "10  [SOMVTRL12A67AE0921, SOAMPRJ12A8AE45F38, SOVHV...  \n",
      "11  [SOWUVFQ12AB018740E, SORMKFZ12A6D4F9395, SOGYN...  \n",
      "12  [SOVVNSS12A58291F72, SOWPAXV12A67ADA046, SOQTR...  \n"
     ]
    }
   ],
   "source": [
    "user_df = triplets_df[:1000][['user_id']].drop_duplicates()\n",
    "print(user_df)\n",
    "\n",
    "predictions = model.predict_all(user_df, k=5)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
